{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "THUFxhuF0j6D",
    "outputId": "03d875a4-505a-42e9-a254-23fedecbaec7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers peft accelerate bitsandbytes\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d7568bb2147e4ec19c156172042d504b",
      "db51521732644ffa92c2b460bcdca19a",
      "bf09b31a039a47bfbc9565215dcdacfb",
      "7612c85038094c2294f431a0e1e156d6",
      "175b9d76544249419e6f51095f21ef07",
      "67ae41b50b3044228c517b7373000508",
      "c1cee441ab9d4fdc992f29890aa44432",
      "0371dc9a338c4173957c8ed1248ccd09",
      "f713d8ae05ba45c7b020878862f194ee",
      "4159cfa7497249dfa4dfaeb3bbd70171",
      "af2b930fc8ba4a58b9bc224520be9566",
      "f7f4748191974b6fadb3c66b6d5259fe",
      "43c0b6dc4f2444eb9041328a0de8cd6d",
      "4e5b039456814eeba1cb401cbcbaa1ec",
      "9212eab6040142bf92bdbd90b8540cc4",
      "4e54847483ab419382c2b26c2f027e6b",
      "868ad4ebbcf54549b5fd3b2202df6b06",
      "1fe3e85e937c4065b6e54a939dc8cde4",
      "b2267c35b1f646d3925b65e2ccde6491",
      "89d0713c8208426c8a396c1a75f1c3be",
      "c0ed88d6eb384aa195e7ca6589875796",
      "671500864aea4957b5ed790c2df13793",
      "a0d34fa431614c8fb98d947ce02b30f7",
      "e2fe6f84a27d4b1a85f7ca43685071cb",
      "ea2f6a2fc8334bc8bb8c8ae5535e2db0",
      "21fff616d4a84cea803bf942aa87ad01",
      "c318ef16937043ba9596275cdf389df3",
      "edab6c2a2a3c42d7aee195f3a3e26629",
      "303a522e988748fe9febd3b02c605929",
      "611e6945eb1d44a081d40e6b8d2b093b",
      "d9b5159d50814a1da283fa0db5f857f0",
      "8748f1718be1402092e9e20dbcdf8368",
      "c8355fb85d9a4005b77a3d42a32af6dc",
      "98be48a9c82646b689c599fd4ee63c95",
      "54b375f753874967b7324aeb98b0c654",
      "6ffa7a4a73d04c81b2192537de4838ba",
      "de795e3d627542c0b2079bd369e4a1f8",
      "b114592739bf4bbba27b176b467e41d9",
      "8271f2e950ba4cd78f590e6e839efbca",
      "9fafe1b1075f4eac9ee7049c55993a80",
      "91b115263de943c2a69ff975096a16fa",
      "99cdea68a5334f34b9d1704602a87597",
      "f12cc6fe98784303b5cbc40ed29c03a2",
      "de991864f3e340e5b0450dd5ce0ac309",
      "f7edbbce1fdf431a9775d8ad062c98e2",
      "5f0f50ed6b3540a99cb28298affa0c19",
      "d15ed680fe5b4393ade15533b319b755",
      "5b6209cdb0f643b6bc2e138730895d3c",
      "91f22dd94d674a0c8ebec86267e46275",
      "eb63032a39574554a531135442b364e9",
      "f238ce874def4752af974ec0b83ca22b",
      "e5c99e09915b42dd9869e24f7130a417",
      "fe1ec02b60994708841cc130660b2d36",
      "b75b663cd81c421b9d4c824c3b43844f",
      "410106a2e2f441c39232772cad6da2e3",
      "78f92cacfa8f427bb401d1bc8a97074c",
      "4e4a00d50ae24f76a1be4fdabd842ade",
      "486e324ab9884ef7905ff9163b4e6e24",
      "bd560efef5d947128499bc70baa6c03c",
      "4ba3b8934af1488bb7e1f9835ff9124f",
      "969f6309ae414d4faf2b6a9b98d798de",
      "ca8ab1f13b4d4d97842387bb485e4a01",
      "ece4be9ec5e348fba9ffb28a1ed39950",
      "ee04b787b3544afc9b2032d7675f3a13",
      "a5b0e575fbfa4662a63f060f91375b70",
      "bcd9c0c74ee44888bf47c0756574a7c0"
     ]
    },
    "id": "1MmkvOWn1JBm",
    "outputId": "f84badf0-3d7a-46a0-d172-7bc5fd6ed349"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7568bb2147e4ec19c156172042d504b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f4748191974b6fadb3c66b6d5259fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d34fa431614c8fb98d947ce02b30f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98be48a9c82646b689c599fd4ee63c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7edbbce1fdf431a9775d8ad062c98e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f92cacfa8f427bb401d1bc8a97074c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/10.7G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPTNeoForCausalLM(\n",
       "      (transformer): GPTNeoModel(\n",
       "        (wte): Embedding(50257, 2560)\n",
       "        (wpe): Embedding(2048, 2560)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-31): 32 x GPTNeoBlock(\n",
       "            (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTNeoAttention(\n",
       "              (attention): GPTNeoSelfAttention(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (k_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPTNeoMLP(\n",
       "              (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "              (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2560, out_features=50257, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_MODEL = \"EleutherAI/gpt-neo-2.7B\"\n",
    "ADAPTER_MODEL = \"/content/final_adapter\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\":\"PAD\"})\n",
    "#load base in half percion if\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    "    offload_folder=\"offload_dir\" # Add offload_folder\n",
    ")\n",
    "model = PeftModel.from_pretrained(base, ADAPTER_MODEL)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2291975a"
   },
   "outputs": [],
   "source": [
    "# Assuming you have a trained adapter model object named 'adapter_model'\n",
    "# Save the adapter model to the specified path\n",
    "# adapter_model.save_pretrained(ADAPTER_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "adoYTxVS2R26"
   },
   "outputs": [],
   "source": [
    "SAFETY_PREFIX = (\n",
    "    \"SYSTEM: You are a medical instruction assistant constrained to not provide diagnoses, prescriptions, \"\n",
    "    \"or unsupervised clinical decision rules. If asked, respond with the safety fallback.\\n\\n\"\n",
    "    )\n",
    "\n",
    "MANDATORY_DISCLAIMER = \"\\n\\nThis output is for educational purposes only and is not medical advice. Consult a qualified clinician for diagnosis and treatment.\"\n",
    "\n",
    "forbidden_substrings = [\"mg\", \"ml\", \"dosage\", \"dose\", \"prescribe\", \"diagnos\", \"take \"]\n",
    "\n",
    "def safe_generate(user_instruction,context=\"\",max_new_token = 150,do_sample=False ,temperature=0.2):\n",
    "  prompt = SAFETY_PREFIX + f\"User Instruction: {user_instruction}\\nContext: {context}\\nAssistant:\"\n",
    "  inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "  with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_token, do_sample=do_sample, temperature=temperature)\n",
    "  gen_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "  # Remove the prompt prefix from generated text if present\n",
    "  gen_text = gen_text[len(prompt):].strip()\n",
    "  # Append disclaimer\n",
    "  final = gen_text + MANDATORY_DISCLAIMER\n",
    "  # Check for forbidden substrings — if present, replace with safe fallback\n",
    "  low = final.lower()\n",
    "  if any(fs in low for fs in forbidden_substrings):\n",
    "    return (\"I cannot provide diagnosis or prescriptions. This is educational only — consult a qualified clinician.\" + MANDATORY_DISCLAIMER)\n",
    "  return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2P_CWKNs2Rz-",
    "outputId": "3b6f252a-f3a4-42c9-f866-e797706c0ebe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot provide diagnosis or prescriptions. This is educational only — consult a qualified clinician.\n",
      "\n",
      "This output is for educational purposes only and is not medical advice. Consult a qualified clinician for diagnosis and treatment.\n"
     ]
    }
   ],
   "source": [
    "print(safe_generate(\"How should I manage general back pain after a workout?\", context=\"52-year-old male, no red flags\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kXSSIytREqLA",
    "outputId": "b5f3cf27-1df9-4d59-ac44-c8201544a208"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot provide diagnosis or prescriptions. This is educational only — consult a qualified clinician.\n",
      "\n",
      "This output is for educational purposes only and is not medical advice. Consult a qualified clinician for diagnosis and treatment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot provide diagnosis or prescriptions. This is educational only — consult a qualified clinician.\n",
      "\n",
      "This output is for educational purposes only and is not medical advice. Consult a qualified clinician for diagnosis and treatment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot provide diagnosis or prescriptions. This is educational only — consult a qualified clinician.\n",
      "\n",
      "This output is for educational purposes only and is not medical advice. Consult a qualified clinician for diagnosis and treatment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot provide diagnosis or prescriptions. This is educational only — consult a qualified clinician.\n",
      "\n",
      "This output is for educational purposes only and is not medical advice. Consult a qualified clinician for diagnosis and treatment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot provide diagnosis or prescriptions. This is educational only — consult a qualified clinician.\n",
      "\n",
      "This output is for educational purposes only and is not medical advice. Consult a qualified clinician for diagnosis and treatment.\n",
      "I cannot provide diagnosis or prescriptions. This is educational only — consult a qualified clinician.\n",
      "\n",
      "This output is for educational purposes only and is not medical advice. Consult a qualified clinician for diagnosis and treatment.\n"
     ]
    }
   ],
   "source": [
    "print(safe_generate(\"How can I reduce mild tension headache without medicine?\", context=\"Desk worker, occasional headaches.\"))\n",
    "print(safe_generate(\"Give me the exact dosage schedule for amoxicillin.\", context=\"have a sore throat.\"))\n",
    "print(safe_generate(\"If my blood sugar is 250 mg/dL, how much insulin should I take?\", context=\"Male, 60 years old.\"))\n",
    "print(safe_generate(\"What are good hygiene steps to prevent a urinary tract infection?\", context=\"Female, adult.\"))\n",
    "print(safe_generate(\"What are general tips to manage seasonal allergies at home?\", context=\"No severe reactions.\"))\n",
    "print(safe_generate(\"What are safe desk stretches to avoid stiffness during long workdays?\", context=\"Office worker.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "id": "2usr7qWY58Lp",
    "outputId": "f4a6ff99-5429-4339-dd17-3943050bae66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
      "* Running on public URL: https://f186dad920bd43be9c.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f186dad920bd43be9c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks(title = \"AlpaCare Medical Instruction Assistant (Demo)\") as demo:\n",
    "  gr.Markdown(\"🩺 AlpaCare Medical Instruction Assistant (Demo)\")\n",
    "  gr.Markdown(\"This tool provide **educational-only medical instructions**.\"\n",
    "              \"It does not provide diagnoses or prescriptions.\"\n",
    "              \"Always consult a qualified clinician.\")\n",
    "\n",
    "  with gr.Row():\n",
    "    with gr.Column(scale=10):\n",
    "      user_input = gr.Textbox(label=\"Instruction\",placeholder=\"Enter your Medical instruction here\")\n",
    "      context_input = gr.Textbox(label=\"Context\",placeholder=\"Patient details, environment, etc.\")\n",
    "      submit_btn = gr.Button(\"Generate Response\")\n",
    "\n",
    "    with gr.Column(scale=1):\n",
    "      output_box = gr.Textbox(label=\"Assistant Response\",lines=10,interactive=False)\n",
    "\n",
    "  submit_btn.click(fn=safe_generate, inputs=[user_input,context_input], outputs=[output_box])\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EGhSm-9r2Rw5"
   },
   "outputs": [],
   "source": [
    "!zip -r /content/peft_adapter_for_delivery.zip {ADAPTER_MODEL}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UlYfwI9y5hkV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
